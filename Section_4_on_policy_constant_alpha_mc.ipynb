{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emarod/Algorithms/blob/main/Section_4_on_policy_constant_alpha_mc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "7gzJnFaonawG"
      },
      "source": [
        "<div style=\"text-align:center\">\n",
        "    <h1>\n",
        "        On-policy Monte Carlo Control\n",
        "    </h1>\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "<div style=\"text-align:center\">\n",
        "    <p>\n",
        "        In this notebook we are going to implement one of the two major strategies that exist when learning a policy by interacting with the environment, called on-policy learning. The agent will perform the task from start to finish and based on the sample experience generated, update its estimates of the q-values of each state-action pair Q(s,a).\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setup code (not important) - Run this cell by pressing \"Shift + Enter\"\n",
        "\n",
        "\n",
        "\n",
        "!pip install -qq gym==0.23.0\n",
        "\n",
        "\n",
        "from typing import Tuple, Dict, Optional, Iterable, Callable\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "from matplotlib import animation\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.error import DependencyNotInstalled\n",
        "\n",
        "import pygame\n",
        "from pygame import gfxdraw\n",
        "\n",
        "\n",
        "class Maze(gym.Env):\n",
        "\n",
        "    def __init__(self, exploring_starts: bool = False,\n",
        "                 shaped_rewards: bool = False, size: int = 5) -> None:\n",
        "        super().__init__()\n",
        "        self.exploring_starts = exploring_starts\n",
        "        self.shaped_rewards = shaped_rewards\n",
        "        self.state = (size - 1, size - 1)\n",
        "        self.goal = (size - 1, size - 1)\n",
        "        self.maze = self._create_maze(size=size)\n",
        "        self.distances = self._compute_distances(self.goal, self.maze)\n",
        "        self.action_space = spaces.Discrete(n=4)\n",
        "        self.action_space.action_meanings = {0: 'UP', 1: 'RIGHT', 2: 'DOWN', 3: \"LEFT\"}\n",
        "        self.observation_space = spaces.MultiDiscrete([size, size])\n",
        "\n",
        "        self.screen = None\n",
        "        self.agent_transform = None\n",
        "\n",
        "    def step(self, action: int) -> Tuple[Tuple[int, int], float, bool, Dict]:\n",
        "        reward = self.compute_reward(self.state, action)\n",
        "        self.state = self._get_next_state(self.state, action)\n",
        "        done = self.state == self.goal\n",
        "        info = {}\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "    def reset(self) -> Tuple[int, int]:\n",
        "        if self.exploring_starts:\n",
        "            while self.state == self.goal:\n",
        "                self.state = tuple(self.observation_space.sample())\n",
        "        else:\n",
        "            self.state = (0, 0)\n",
        "        return self.state\n",
        "\n",
        "    def render(self, mode: str = 'human') -> Optional[np.ndarray]:\n",
        "        assert mode in ['human', 'rgb_array']\n",
        "\n",
        "        screen_size = 600\n",
        "        scale = screen_size / 5\n",
        "\n",
        "        if self.screen is None:\n",
        "            pygame.init()\n",
        "            self.screen = pygame.Surface((screen_size, screen_size))\n",
        "\n",
        "        surf = pygame.Surface((screen_size, screen_size))\n",
        "        surf.fill((22, 36, 71))\n",
        "\n",
        "\n",
        "        for row in range(5):\n",
        "            for col in range(5):\n",
        "\n",
        "                state = (row, col)\n",
        "                for next_state in [(row + 1, col), (row - 1, col), (row, col + 1), (row, col - 1)]:\n",
        "                    if next_state not in self.maze[state]:\n",
        "\n",
        "                        # Add the geometry of the edges and walls (i.e. the boundaries between\n",
        "                        # adjacent squares that are not connected).\n",
        "                        row_diff, col_diff = np.subtract(next_state, state)\n",
        "                        left = (col + (col_diff > 0)) * scale - 2 * (col_diff != 0)\n",
        "                        right = ((col + 1) - (col_diff < 0)) * scale + 2 * (col_diff != 0)\n",
        "                        top = (5 - (row + (row_diff > 0))) * scale - 2 * (row_diff != 0)\n",
        "                        bottom = (5 - ((row + 1) - (row_diff < 0))) * scale + 2 * (row_diff != 0)\n",
        "\n",
        "                        gfxdraw.filled_polygon(surf, [(left, bottom), (left, top), (right, top), (right, bottom)], (255, 255, 255))\n",
        "\n",
        "        # Add the geometry of the goal square to the viewer.\n",
        "        left, right, top, bottom = scale * 4 + 10, scale * 5 - 10, scale - 10, 10\n",
        "        gfxdraw.filled_polygon(surf, [(left, bottom), (left, top), (right, top), (right, bottom)], (40, 199, 172))\n",
        "\n",
        "        # Add the geometry of the agent to the viewer.\n",
        "        agent_row = int(screen_size - scale * (self.state[0] + .5))\n",
        "        agent_col = int(scale * (self.state[1] + .5))\n",
        "        gfxdraw.filled_circle(surf, agent_col, agent_row, int(scale * .6 / 2), (228, 63, 90))\n",
        "\n",
        "        surf = pygame.transform.flip(surf, False, True)\n",
        "        self.screen.blit(surf, (0, 0))\n",
        "\n",
        "        return np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "    def close(self) -> None:\n",
        "        if self.screen is not None:\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n",
        "            self.screen = None\n",
        "\n",
        "    def compute_reward(self, state: Tuple[int, int], action: int) -> float:\n",
        "        next_state = self._get_next_state(state, action)\n",
        "        if self.shaped_rewards:\n",
        "            return - (self.distances[next_state] / self.distances.max())\n",
        "        return - float(state != self.goal)\n",
        "\n",
        "    def simulate_step(self, state: Tuple[int, int], action: int):\n",
        "        reward = self.compute_reward(state, action)\n",
        "        next_state = self._get_next_state(state, action)\n",
        "        done = next_state == self.goal\n",
        "        info = {}\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "    def _get_next_state(self, state: Tuple[int, int], action: int) -> Tuple[int, int]:\n",
        "        if action == 0:\n",
        "            next_state = (state[0] - 1, state[1])\n",
        "        elif action == 1:\n",
        "            next_state = (state[0], state[1] + 1)\n",
        "        elif action == 2:\n",
        "            next_state = (state[0] + 1, state[1])\n",
        "        elif action == 3:\n",
        "            next_state = (state[0], state[1] - 1)\n",
        "        else:\n",
        "            raise ValueError(\"Action value not supported:\", action)\n",
        "        if next_state in self.maze[state]:\n",
        "            return next_state\n",
        "        return state\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_maze(size: int) -> Dict[Tuple[int, int], Iterable[Tuple[int, int]]]:\n",
        "        maze = {(row, col): [(row - 1, col), (row + 1, col), (row, col - 1), (row, col + 1)]\n",
        "                for row in range(size) for col in range(size)}\n",
        "\n",
        "        left_edges = [[(row, 0), (row, -1)] for row in range(size)]\n",
        "        right_edges = [[(row, size - 1), (row, size)] for row in range(size)]\n",
        "        upper_edges = [[(0, col), (-1, col)] for col in range(size)]\n",
        "        lower_edges = [[(size - 1, col), (size, col)] for col in range(size)]\n",
        "        walls = [\n",
        "            [(1, 0), (1, 1)], [(2, 0), (2, 1)], [(3, 0), (3, 1)],\n",
        "            [(1, 1), (1, 2)], [(2, 1), (2, 2)], [(3, 1), (3, 2)],\n",
        "            [(3, 1), (4, 1)], [(0, 2), (1, 2)], [(1, 2), (1, 3)],\n",
        "            [(2, 2), (3, 2)], [(2, 3), (3, 3)], [(2, 4), (3, 4)],\n",
        "            [(4, 2), (4, 3)], [(1, 3), (1, 4)], [(2, 3), (2, 4)],\n",
        "        ]\n",
        "\n",
        "        obstacles = upper_edges + lower_edges + left_edges + right_edges + walls\n",
        "\n",
        "        for src, dst in obstacles:\n",
        "            maze[src].remove(dst)\n",
        "\n",
        "            if dst in maze:\n",
        "                maze[dst].remove(src)\n",
        "\n",
        "        return maze\n",
        "\n",
        "    @staticmethod\n",
        "    def _compute_distances(goal: Tuple[int, int],\n",
        "                           maze: Dict[Tuple[int, int], Iterable[Tuple[int, int]]]) -> np.ndarray:\n",
        "        distances = np.full((5, 5), np.inf)\n",
        "        visited = set()\n",
        "        distances[goal] = 0.\n",
        "\n",
        "        while visited != set(maze):\n",
        "            sorted_dst = [(v // 5, v % 5) for v in distances.argsort(axis=None)]\n",
        "            closest = next(x for x in sorted_dst if x not in visited)\n",
        "            visited.add(closest)\n",
        "\n",
        "            for neighbour in maze[closest]:\n",
        "                distances[neighbour] = min(distances[neighbour], distances[closest] + 1)\n",
        "        return distances\n",
        "\n",
        "\n",
        "def plot_policy(probs_or_qvals, frame, action_meanings=None):\n",
        "    if action_meanings is None:\n",
        "        action_meanings = {0: 'U', 1: 'R', 2: 'D', 3: 'L'}\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "    max_prob_actions = probs_or_qvals.argmax(axis=-1)\n",
        "    probs_copy = max_prob_actions.copy().astype(object)\n",
        "    for key in action_meanings:\n",
        "        probs_copy[probs_copy == key] = action_meanings[key]\n",
        "    sns.heatmap(max_prob_actions, annot=probs_copy, fmt='', cbar=False, cmap='coolwarm',\n",
        "                annot_kws={'weight': 'bold', 'size': 12}, linewidths=2, ax=axes[0])\n",
        "    axes[1].imshow(frame)\n",
        "    axes[0].axis('off')\n",
        "    axes[1].axis('off')\n",
        "    plt.suptitle(\"Policy\", size=18)\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "def plot_values(state_values, frame):\n",
        "    f, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    sns.heatmap(state_values, annot=True, fmt=\".2f\", cmap='coolwarm',\n",
        "                annot_kws={'weight': 'bold', 'size': 12}, linewidths=2, ax=axes[0])\n",
        "    axes[1].imshow(frame)\n",
        "    axes[0].axis('off')\n",
        "    axes[1].axis('off')\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "def display_video(frames):\n",
        "    # Copied from: https://colab.research.google.com/github/deepmind/dm_control/blob/master/tutorial.ipynb\n",
        "    orig_backend = matplotlib.get_backend()\n",
        "    matplotlib.use('Agg')\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
        "    matplotlib.use(orig_backend)\n",
        "    ax.set_axis_off()\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_position([0, 0, 1, 1])\n",
        "    im = ax.imshow(frames[0])\n",
        "    def update(frame):\n",
        "        im.set_data(frame)\n",
        "        return [im]\n",
        "    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
        "                                    interval=50, blit=True, repeat=False)\n",
        "    return HTML(anim.to_html5_video())\n",
        "\n",
        "\n",
        "def test_agent(environment, policy, episodes=10):\n",
        "    frames = []\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        frames.append(env.render(mode=\"rgb_array\"))\n",
        "\n",
        "        while not done:\n",
        "            p = policy(state)\n",
        "            if isinstance(p, np.ndarray):\n",
        "                action = np.random.choice(4, p=p)\n",
        "            else:\n",
        "                action = p\n",
        "            next_state, reward, done, extra_info = env.step(action)\n",
        "            img = env.render(mode=\"rgb_array\")\n",
        "            frames.append(img)\n",
        "            state = next_state\n",
        "\n",
        "    return display_video(frames)\n",
        "\n",
        "\n",
        "def plot_action_values(action_values):\n",
        "\n",
        "    text_positions = [\n",
        "        [(0.35, 4.75), (1.35, 4.75), (2.35, 4.75), (3.35, 4.75), (4.35, 4.75),\n",
        "         (0.35, 3.75), (1.35, 3.75), (2.35, 3.75), (3.35, 3.75), (4.35, 3.75),\n",
        "         (0.35, 2.75), (1.35, 2.75), (2.35, 2.75), (3.35, 2.75), (4.35, 2.75),\n",
        "         (0.35, 1.75), (1.35, 1.75), (2.35, 1.75), (3.35, 1.75), (4.35, 1.75),\n",
        "         (0.35, 0.75), (1.35, 0.75), (2.35, 0.75), (3.35, 0.75), (4.35, 0.75)],\n",
        "        [(0.6, 4.45), (1.6, 4.45), (2.6, 4.45), (3.6, 4.45), (4.6, 4.45),\n",
        "         (0.6, 3.45), (1.6, 3.45), (2.6, 3.45), (3.6, 3.45), (4.6, 3.45),\n",
        "         (0.6, 2.45), (1.6, 2.45), (2.6, 2.45), (3.6, 2.45), (4.6, 2.45),\n",
        "         (0.6, 1.45), (1.6, 1.45), (2.6, 1.45), (3.6, 1.45), (4.6, 1.45),\n",
        "         (0.6, 0.45), (1.6, 0.45), (2.6, 0.45), (3.6, 0.45), (4.6, 0.45)],\n",
        "        [(0.35, 4.15), (1.35, 4.15), (2.35, 4.15), (3.35, 4.15), (4.35, 4.15),\n",
        "         (0.35, 3.15), (1.35, 3.15), (2.35, 3.15), (3.35, 3.15), (4.35, 3.15),\n",
        "         (0.35, 2.15), (1.35, 2.15), (2.35, 2.15), (3.35, 2.15), (4.35, 2.15),\n",
        "         (0.35, 1.15), (1.35, 1.15), (2.35, 1.15), (3.35, 1.15), (4.35, 1.15),\n",
        "         (0.35, 0.15), (1.35, 0.15), (2.35, 0.15), (3.35, 0.15), (4.35, 0.15)],\n",
        "        [(0.05, 4.45), (1.05, 4.45), (2.05, 4.45), (3.05, 4.45), (4.05, 4.45),\n",
        "         (0.05, 3.45), (1.05, 3.45), (2.05, 3.45), (3.05, 3.45), (4.05, 3.45),\n",
        "         (0.05, 2.45), (1.05, 2.45), (2.05, 2.45), (3.05, 2.45), (4.05, 2.45),\n",
        "         (0.05, 1.45), (1.05, 1.45), (2.05, 1.45), (3.05, 1.45), (4.05, 1.45),\n",
        "         (0.05, 0.45), (1.05, 0.45), (2.05, 0.45), (3.05, 0.45), (4.05, 0.45)]]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(7, 7))\n",
        "    tripcolor = quatromatrix(action_values, ax=ax,\n",
        "                             triplotkw={\"color\": \"k\", \"lw\": 1}, tripcolorkw={\"cmap\": \"coolwarm\"})\n",
        "    ax.margins(0)\n",
        "    ax.set_aspect(\"equal\")\n",
        "    fig.colorbar(tripcolor)\n",
        "\n",
        "    for j, av in enumerate(text_positions):\n",
        "        for i, (xi, yi) in enumerate(av):\n",
        "            plt.text(xi, yi, round(action_values[:, :, j].flatten()[i], 2), size=8, color=\"w\", weight=\"bold\")\n",
        "\n",
        "    plt.title(\"Action values Q(s,a)\", size=18)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def quatromatrix(action_values, ax=None, triplotkw=None, tripcolorkw=None):\n",
        "    action_values = np.flipud(action_values)\n",
        "    n = 5\n",
        "    m = 5\n",
        "    a = np.array([[0, 0], [0, 1], [.5, .5], [1, 0], [1, 1]])\n",
        "    tr = np.array([[0, 1, 2], [0, 2, 3], [2, 3, 4], [1, 2, 4]])\n",
        "    A = np.zeros((n * m * 5, 2))\n",
        "    Tr = np.zeros((n * m * 4, 3))\n",
        "    for i in range(n):\n",
        "        for j in range(m):\n",
        "            k = i * m + j\n",
        "            A[k * 5:(k + 1) * 5, :] = np.c_[a[:, 0] + j, a[:, 1] + i]\n",
        "            Tr[k * 4:(k + 1) * 4, :] = tr + k * 5\n",
        "    C = np.c_[action_values[:, :, 3].flatten(), action_values[:, :, 2].flatten(),\n",
        "              action_values[:, :, 1].flatten(), action_values[:, :, 0].flatten()].flatten()\n",
        "\n",
        "    ax.triplot(A[:, 0], A[:, 1], Tr, **triplotkw)\n",
        "    tripcolor = ax.tripcolor(A[:, 0], A[:, 1], Tr, facecolors=C, **tripcolorkw)\n",
        "    return tripcolor\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "LyTAkpthoZMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63oAK3JLnawK"
      },
      "source": [
        "## Import the necessary software libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oYNpIN1nawK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtHqOrD4nawL"
      },
      "source": [
        "## Initialize the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROYlzzQLnawM"
      },
      "outputs": [],
      "source": [
        "env = Maze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_by7VW9nawM"
      },
      "outputs": [],
      "source": [
        "frame = env.render(mode='rgb_array')\n",
        "plt.axis('off')\n",
        "plt.imshow(frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mJuSuvmnawM"
      },
      "outputs": [],
      "source": [
        "print(f\"Observation space shape: {env.observation_space.nvec}\")\n",
        "print(f\"Number of actions: {env.action_space.n}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI4LgZwfnawN"
      },
      "source": [
        "## Define value table $Q(s, a)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Dl-VI9vnawN"
      },
      "source": [
        "#### Create the $Q(s, a)$ table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Enbb5n6GnawN"
      },
      "outputs": [],
      "source": [
        "action_values = np.zeros(shape=(5, 5, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2HpoEzanawN"
      },
      "source": [
        "#### Plot Q(s, a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlEzZ1ZsnawO"
      },
      "outputs": [],
      "source": [
        "plot_action_values(action_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcCVmpfmnawO"
      },
      "source": [
        "## Define the policy $\\pi(s)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5LVZQSGnawO"
      },
      "source": [
        "#### Create the policy $\\pi(s)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlphKjJtnawO"
      },
      "outputs": [],
      "source": [
        "def policy(state, epsilon=0.99):\n",
        "    if np.random.random() < epsilon:\n",
        "        return np.random.randint(4)\n",
        "    else:\n",
        "        av = action_values[state]\n",
        "        return np.random.choice(np.flatnonzero(av == av.max()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk4HCqGZnawO"
      },
      "source": [
        "#### Test the policy with state (0, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_zxUCWAnawO"
      },
      "outputs": [],
      "source": [
        "action = policy((0,0))\n",
        "print(f\"Action taken in state (0,0): {action}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGkcRgLknawO"
      },
      "source": [
        "#### Plot the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "scrolled": false,
        "id": "o2wPlGyvnawO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "outputId": "0e3c221d-6fd6-4242-f0b3-afefaaae2e3e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plot_policy(action_values, frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVow816ZnawP"
      },
      "source": [
        "## Implement the algorithm\n",
        "\n",
        "</br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oS6xb3hlnawP"
      },
      "outputs": [],
      "source": [
        "def on_policy_constant_alpha_mc(policy, action_values, episodes, gamma=0.99, epsilon=0.2, alpha=0.1):\n",
        "  qa_returns = {}\n",
        "  for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    while not(done):\n",
        "      next_state, reward, done, info = env.step(policy(state, epsilon))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIObBa_6nawP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APkcdvo7nawP"
      },
      "source": [
        "## Show results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKwSTqbJnawP"
      },
      "source": [
        "#### Show resulting value table $Q(s, a)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "di0-Q_cznawP"
      },
      "outputs": [],
      "source": [
        "plot_action_values(action_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbl9SUNKnawP"
      },
      "source": [
        "#### Show resulting policy $\\pi(\\cdot|s)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwZcbTiTnawP"
      },
      "outputs": [],
      "source": [
        "plot_policy(action_values, frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-F5qY5DgnawP"
      },
      "source": [
        "#### Test the resulting agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PpECiz5nawP"
      },
      "outputs": [],
      "source": [
        "test_agent(env, policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u60cRVCqnawP"
      },
      "source": [
        "## Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJY7tkIQnawP"
      },
      "source": [
        "[[1] Reinforcement Learning: An Introduction. Ch. 4: Dynamic Programming](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}